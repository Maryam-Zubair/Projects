{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!pip install spacy_transformers"
      ],
      "metadata": {
        "id": "ew35Qxo2gipp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pickle"
      ],
      "metadata": {
        "id": "pcE2Zx2_UEZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "import json\n",
        "import re\n",
        "\n",
        "# JSON formatting functions\n",
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    training_data = []\n",
        "    lines=[]\n",
        "    with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        data = json.loads(line)\n",
        "        text = data['content'].replace(\"\\n\", \" \")\n",
        "        entities = []\n",
        "        data_annotations = data['annotation']\n",
        "        if data_annotations is not None:\n",
        "            for annotation in data_annotations:\n",
        "                #only a single point in text annotation.\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                # handle both list of labels or a single label.\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "\n",
        "                for label in labels:\n",
        "                    point_start = point['start']\n",
        "                    point_end = point['end']\n",
        "                    point_text = point['text']\n",
        "\n",
        "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                    if lstrip_diff != 0:\n",
        "                        point_start = point_start + lstrip_diff\n",
        "                    if rstrip_diff != 0:\n",
        "                        point_end = point_end - rstrip_diff\n",
        "                    entities.append((point_start, point_end + 1 , label))\n",
        "        training_data.append((text, {\"entities\" : entities}))\n",
        "    return training_data\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "    return cleaned_data"
      ],
      "metadata": {
        "id": "h3DLQIuWyREJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = trim_entity_spans(convert_dataturks_to_spacy(\"/content/Entity Recognition in Resumes.json\"))\n",
        "data[0]"
      ],
      "metadata": {
        "id": "iOv8nwS-yVXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51de4734-a337-4adc-c4fb-c035df33aa18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\",\n",
              " {'entities': [[1296, 1622, 'Skills'],\n",
              "   [993, 1154, 'Skills'],\n",
              "   [939, 957, 'College Name'],\n",
              "   [883, 905, 'College Name'],\n",
              "   [856, 860, 'Graduation Year'],\n",
              "   [771, 814, 'College Name'],\n",
              "   [727, 769, 'Designation'],\n",
              "   [407, 416, 'Companies worked at'],\n",
              "   [372, 405, 'Designation'],\n",
              "   [95, 145, 'Email Address'],\n",
              "   [60, 69, 'Location'],\n",
              "   [49, 58, 'Companies worked at'],\n",
              "   [13, 46, 'Designation'],\n",
              "   [0, 12, 'Name']]}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "da"
      ],
      "metadata": {
        "id": "RLxVK-c2Tmyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "3vlTvBwzzwfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_path = '/content/output'\n",
        "\n",
        "# # Use the !rm command to remove the file\n",
        "# !rm -r \"{file_path}\""
      ],
      "metadata": {
        "id": "0N-5CQlHw8zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = DocBin()\n",
        "i = 0\n",
        "for text, annot in tqdm(data):\n",
        "    try:\n",
        "        doc = nlp.make_doc(text)\n",
        "        ents = []\n",
        "        for start, end, label in annot['entities']:\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
        "            if span is None:\n",
        "                print('Skipping entity')\n",
        "            else:\n",
        "                ents.append(span)\n",
        "        doc.ents = ents\n",
        "        db.add(doc)\n",
        "        i += 1\n",
        "        print('loaded', i)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example: {e}\")\n",
        "\n",
        "# Print the total number of resumes processed\n",
        "print(f'Total resumes processed: {i}')"
      ],
      "metadata": {
        "id": "Y896868UUpr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db"
      ],
      "metadata": {
        "id": "A8vRZZsN8kAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, test_size=0.1 )"
      ],
      "metadata": {
        "id": "2vihEmle0eei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.to_disk('train_data.spacy')\n",
        "db.to_disk('test_data.spacy')"
      ],
      "metadata": {
        "id": "5rgsSMFl0sEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/base_config.cfg /content/config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7B4JqQBfRST",
        "outputId": "406b96e3-576e-4164-9700-de9b41c7c4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-01 05:14:21.075658: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-01 05:14:21.075713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-01 05:14:21.076924: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-01 05:14:22.096575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AK-DZbDgtOC",
        "outputId": "9949117a-e3c2-477d-b437-5b0ba4b842e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-01 05:17:24.834770: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-01 05:17:24.834814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-01 05:17:24.836005: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-01 05:17:25.943713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        5020.57   1432.15    0.08    0.04    1.48    0.00\n",
            "  3     200      139989.09  64783.27   40.77   51.48   33.75    0.41\n",
            "  7     400       43234.14  23780.02   67.98   63.51   73.14    0.68\n",
            " 11     600       12888.52  19906.57   80.51   77.69   83.54    0.81\n",
            " 15     800        5145.37  18072.24   72.31   62.63   85.53    0.72\n",
            " 19    1000        7323.27  17067.18   89.99   89.60   90.38    0.90\n",
            " 23    1200       28052.00  16068.63   92.92   93.37   92.46    0.93\n",
            " 27    1400        5313.35  15185.43   91.47   89.30   93.76    0.91\n",
            " 31    1600         951.39  14038.64   96.09   96.63   95.56    0.96\n",
            " 35    1800         573.38  13414.96   95.52   95.52   95.52    0.96\n",
            " 39    2000        6064.02  13105.23   97.09   96.87   97.32    0.97\n",
            " 43    2200         653.77  12454.65   96.17   94.83   97.55    0.96\n",
            " 47    2400        1849.48  11776.47   97.32   97.10   97.55    0.97\n",
            " 50    2600         340.48  11098.93   97.33   97.06   97.60    0.97\n",
            " 54    2800         224.87   9968.01   97.78   98.00   97.55    0.98\n",
            " 58    3000         318.85   9264.71   97.82   98.00   97.64    0.98\n",
            " 62    3200         260.19   8461.36   98.08   98.28   97.87    0.98\n",
            " 66    3400         194.23   7394.50   98.01   97.97   98.06    0.98\n",
            " 70    3600         194.23   6286.27   97.81   98.41   97.23    0.98\n",
            " 74    3800        3713.14   5212.03   97.16   96.15   98.20    0.97\n",
            " 78    4000         222.45   4030.83   97.97   97.97   97.97    0.98\n",
            " 82    4200       33455.03   3239.75   97.94   97.88   98.01    0.98\n",
            " 86    4400         399.70   2164.41   97.91   98.37   97.46    0.98\n",
            " 90    4600         154.56   1439.82   98.00   98.69   97.32    0.98\n",
            " 94    4800        1872.78   1037.37   98.11   98.02   98.20    0.98\n",
            " 98    5000         241.23    679.99   98.08   97.93   98.24    0.98\n",
            "101    5200         280.33    436.07   98.17   98.47   97.87    0.98\n",
            "105    5400         196.79    299.17   98.19   98.33   98.06    0.98\n",
            "\n",
            "\u001b[31mAborted.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/main_file.zip /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E73-2SLXcGDv",
        "outputId": "75d340bf-c7c7-4f33-b688-a96574c55708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/ (stored 0%)\n",
            "  adding: content/output/model-best/ (stored 0%)\n",
            "  adding: content/output/model-best/meta.json (deflated 68%)\n",
            "  adding: content/output/model-best/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-best/ner/ (stored 0%)\n",
            "  adding: content/output/model-best/ner/model (deflated 8%)\n",
            "  adding: content/output/model-best/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-best/ner/moves (deflated 73%)\n",
            "  adding: content/output/model-best/vocab/ (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-best/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-best/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-best/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-best/vocab/strings.json (deflated 76%)\n",
            "  adding: content/output/model-best/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-best/transformer/ (stored 0%)\n",
            "  adding: content/output/model-best/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-best/transformer/cfg (stored 0%)\n",
            "  adding: content/output/model-last/ (stored 0%)\n",
            "  adding: content/output/model-last/meta.json (deflated 68%)\n",
            "  adding: content/output/model-last/config.cfg (deflated 61%)\n",
            "  adding: content/output/model-last/ner/ (stored 0%)\n",
            "  adding: content/output/model-last/ner/model (deflated 8%)\n",
            "  adding: content/output/model-last/ner/cfg (deflated 33%)\n",
            "  adding: content/output/model-last/ner/moves (deflated 73%)\n",
            "  adding: content/output/model-last/vocab/ (stored 0%)\n",
            "  adding: content/output/model-last/vocab/vectors (deflated 45%)\n",
            "  adding: content/output/model-last/vocab/key2row (stored 0%)\n",
            "  adding: content/output/model-last/vocab/lookups.bin (stored 0%)\n",
            "  adding: content/output/model-last/vocab/vectors.cfg (stored 0%)\n",
            "  adding: content/output/model-last/vocab/strings.json (deflated 76%)\n",
            "  adding: content/output/model-last/tokenizer (deflated 81%)\n",
            "  adding: content/output/model-last/transformer/ (stored 0%)\n",
            "  adding: content/output/model-last/transformer/model (deflated 13%)\n",
            "  adding: content/output/model-last/transformer/cfg (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "XyaKbVWPeA1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz"
      ],
      "metadata": {
        "id": "SbK0a_TjeIBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc[page_num]\n",
        "        # converting each page into text\n",
        "        text += page.get_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "9h7uBJxSdPsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path='/content/AWAIS ZUBAIR.pdf'\n",
        "resume_text = extract_text_from_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "AHfOkHnldREp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = resume_text.strip()"
      ],
      "metadata": {
        "id": "tpfvPaMudfCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(text.split())"
      ],
      "metadata": {
        "id": "YSiDUOETdfAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(r\"./output/model-best\")"
      ],
      "metadata": {
        "id": "aveu35S1edBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy validate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mKXPen5tlHG",
        "outputId": "20db3a41-f59e-49d5-9f0f-760b0a16d15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-01 08:08:42.591744: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-01 08:08:42.591791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-01 08:08:42.595872: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-01 08:08:44.770891: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
            "\u001b[1m\n",
            "================= Installed pipeline packages (spaCy v3.7.2) =================\u001b[0m\n",
            "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.10/dist-packages/spacy\u001b[0m\n",
            "\n",
            "NAME             SPACY            VERSION                       \n",
            "en_core_web_sm   >=3.6.0,<3.7.0   \u001b[38;5;3m3.6.0\u001b[0m   --> 3.7.1\n",
            "\n",
            "\u001b[1m\n",
            "============================== Install updates ==============================\u001b[0m\n",
            "Use the following commands to update the packages:\n",
            "python -m spacy download en_core_web_sm\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_entities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHSxAi2Tzahb",
        "outputId": "85ae3175-133f-48e3-cfff-a3231ecc430e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Maryam Zubair', 'Name'),\n",
              " ('Fremont', 'Location'),\n",
              " ('mariyam051@gmail', 'Email Address'),\n",
              " ('Master of Science (M.S.), Computer Science, GPA: 3.7', 'Degree'),\n",
              " ('San Francisco Bay University', 'College Name'),\n",
              " ('Fremont', 'Location'),\n",
              " ('Bachelor of Science (B.S.), Electrical Telecommunications Engineering',\n",
              "  'Degree'),\n",
              " ('COMSATS Institute of Information Technology', 'College Name'),\n",
              " ('2016', 'Graduation Year'),\n",
              " ('Languages: Python, JavaScript, SQL \\nFrameworks: Express, Flask, LangChain \\nLibraries: scikit-learn, Pandas, NumPy \\nMachine Learning & Artificial Intelligence: Experienced training machine learning models, developing \\nAI-powered applications utilizing OpenAI APIs, and designing efficient pipelines. \\nData Analysis and Visualization: Proficient in creating dashboards using Power BI  \\nPROJECTS \\nAI-powered Chatbot (Flask, OpenAI API, LangChain, ChromaDB) \\n• \\nDesigned fully functional multilingual chat application responsible to assist prospective students \\nand staff by providing real-time interactive communication. \\n• \\nUtilized chromaDB for retrieving accurate responses, integrated Google-Text-to-Speech (GTTS) \\nand Whisper for seamless voice input/output and deployed chatbot using Flask. \\n• \\nSubsequently achieved 95% accuracy by Fine-tuning the RAG model to better align it with \\nuniversity’s specific requirements. \\nRecommendation System (Python, Flask)  \\n• \\nDeveloped movie recommender system using Python. The application features a Flask web \\ninterface, allowing users to receive movie recommendations by inputting their preferred titles.  \\nOrder Tracking Application (Express, Node.js, RESTful API) \\n• \\nDeveloped Express application featuring RESTful APIs designed to enable users to track their \\norders using an external API provided by DHL.',\n",
              "  'Skills')]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    }
  ]
}